{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d163bb7-c6f8-4780-b1f7-b8ff3252f5f1",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "* Çoğu ortamda çalışabilir\n",
    "    * Python\n",
    "    * Java\n",
    "    * R\n",
    "    * Julia\n",
    "    * Scala\n",
    "* 3 önemli özelliği\n",
    "    * Yüksek verilerde iyi performans gösterir (Çalışma performansı)\n",
    "    * Hızlı çalışma\n",
    "    * Problem ve modelim yorumunun mümkün olması\n",
    "* Kaynak/Kurulum: xgboost.readthedocs.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e989f672-2f0c-4a9a-ab5c-a42abd6af742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe36b70e-9429-40cd-adcb-629b2199721a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'categorical_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12696\\2520026760.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mohe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mohe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'categorical_features'"
     ]
    }
   ],
   "source": [
    "veriler = pd.read_csv(\"../Docs/Churn_Modelling.csv\")\n",
    "\n",
    "# Veri ön işleme\n",
    "\n",
    "X= veriler.iloc[:,3:13].values\n",
    "Y = veriler.iloc[:,13].values\n",
    "\n",
    "\n",
    "#encoder: Kategorik -> Numeric\n",
    "\n",
    "le = LabelEncoder()\n",
    "X[:,1] = le.fit_transform(X[:,1])\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "X[:,2] = le2.fit_transform(X[:,2])\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features = [1])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "X = X[:,1:]\n",
    "\n",
    "# Verilerin eğitim ve test için bölünmesi\n",
    "\n",
    "x_train, x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d2c0eb4-cecc-486f-aac7-31685af19a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:55:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[[1491  187]\n",
      " [ 104  218]]\n"
     ]
    }
   ],
   "source": [
    "classifier = XGBClassifier()\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "cm = confusion_matrix(y_pred, y_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "648e12a8-249c-4365-b8df-12289183af8b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        XGBClassifier\n",
       "\u001b[1;31mString form:\u001b[0m\n",
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "           colsample_byno <...> _weight=1, subsample=1,\n",
       "           tree_method='exact', validate_parameters=1, verbosity=None)\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\tolga\\appdata\\local\\programs\\python\\python310\\lib\\site-packages\\xgboost\\sklearn.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Implementation of the scikit-learn API for XGBoost classification.\n",
       "\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "    n_estimators : int\n",
       "        Number of boosting rounds.\n",
       "    use_label_encoder : bool\n",
       "        (Deprecated) Use the label encoder from scikit-learn to encode the labels. For new\n",
       "        code, we recommend that you set this parameter to False.\n",
       "\n",
       "    max_depth :  Optional[int]\n",
       "        Maximum tree depth for base learners.\n",
       "    learning_rate : Optional[float]\n",
       "        Boosting learning rate (xgb's \"eta\")\n",
       "    verbosity : Optional[int]\n",
       "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
       "    objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
       "        Specify the learning task and the corresponding learning objective or\n",
       "        a custom objective function to be used (see note below).\n",
       "    booster: Optional[str]\n",
       "        Specify which booster to use: gbtree, gblinear or dart.\n",
       "    tree_method: Optional[str]\n",
       "        Specify which tree method to use.  Default to auto.  If this parameter\n",
       "        is set to default, XGBoost will choose the most conservative option\n",
       "        available.  It's recommended to study this option from the parameters\n",
       "        document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
       "    n_jobs : Optional[int]\n",
       "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
       "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
       "        balance the threads.  Creating thread contention will significantly slow down both\n",
       "        algorithms.\n",
       "    gamma : Optional[float]\n",
       "        Minimum loss reduction required to make a further partition on a leaf\n",
       "        node of the tree.\n",
       "    min_child_weight : Optional[float]\n",
       "        Minimum sum of instance weight(hessian) needed in a child.\n",
       "    max_delta_step : Optional[float]\n",
       "        Maximum delta step we allow each tree's weight estimation to be.\n",
       "    subsample : Optional[float]\n",
       "        Subsample ratio of the training instance.\n",
       "    colsample_bytree : Optional[float]\n",
       "        Subsample ratio of columns when constructing each tree.\n",
       "    colsample_bylevel : Optional[float]\n",
       "        Subsample ratio of columns for each level.\n",
       "    colsample_bynode : Optional[float]\n",
       "        Subsample ratio of columns for each split.\n",
       "    reg_alpha : Optional[float]\n",
       "        L1 regularization term on weights (xgb's alpha).\n",
       "    reg_lambda : Optional[float]\n",
       "        L2 regularization term on weights (xgb's lambda).\n",
       "    scale_pos_weight : Optional[float]\n",
       "        Balancing of positive and negative weights.\n",
       "    base_score : Optional[float]\n",
       "        The initial prediction score of all instances, global bias.\n",
       "    random_state : Optional[Union[numpy.random.RandomState, int]]\n",
       "        Random number seed.\n",
       "\n",
       "        .. note::\n",
       "\n",
       "           Using gblinear booster with shotgun updater is nondeterministic as\n",
       "           it uses Hogwild algorithm.\n",
       "\n",
       "    missing : float, default np.nan\n",
       "        Value in the data which needs to be present as a missing value.\n",
       "    num_parallel_tree: Optional[int]\n",
       "        Used for boosting random forest.\n",
       "    monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
       "        Constraint of variable monotonicity.  See tutorial for more\n",
       "        information.\n",
       "    interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
       "        Constraints for interaction representing permitted interactions.  The\n",
       "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
       "        [2, 3, 4]], where each inner list is a group of indices of features\n",
       "        that are allowed to interact with each other.  See tutorial for more\n",
       "        information\n",
       "    importance_type: Optional[str]\n",
       "        The feature importance type for the feature_importances\\_ property:\n",
       "\n",
       "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
       "          \"total_cover\".\n",
       "        * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
       "          without bias.\n",
       "\n",
       "    gpu_id : Optional[int]\n",
       "        Device ordinal.\n",
       "    validate_parameters : Optional[bool]\n",
       "        Give warnings for unknown parameter.\n",
       "    predictor : Optional[str]\n",
       "        Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
       "        gpu_predictor].\n",
       "    enable_categorical : bool\n",
       "\n",
       "        .. versionadded:: 1.5.0\n",
       "\n",
       "        Experimental support for categorical data.  Do not set to true unless you are\n",
       "        interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
       "\n",
       "    kwargs : dict, optional\n",
       "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
       "        parameters can be found here:\n",
       "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
       "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
       "        dict simultaneously will result in a TypeError.\n",
       "\n",
       "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
       "\n",
       "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
       "            that parameters passed via this argument will interact properly\n",
       "            with scikit-learn.\n",
       "\n",
       "        .. note::  Custom objective function\n",
       "\n",
       "            A custom objective function can be provided for the ``objective``\n",
       "            parameter. In this case, it should have the signature\n",
       "            ``objective(y_true, y_pred) -> grad, hess``:\n",
       "\n",
       "            y_true: array_like of shape [n_samples]\n",
       "                The target values\n",
       "            y_pred: array_like of shape [n_samples]\n",
       "                The predicted values\n",
       "\n",
       "            grad: array_like of shape [n_samples]\n",
       "                The value of the gradient for each sample point.\n",
       "            hess: array_like of shape [n_samples]\n",
       "                The value of the second derivative for each sample point\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc77b40-8be3-43ab-8363-7143e1b73d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
